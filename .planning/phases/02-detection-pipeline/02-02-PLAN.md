---
phase: 02-detection-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - offscreen/offscreen.html
  - offscreen/offscreen.js
  - background.js
autonomous: false

must_haves:
  truths:
    - "Distance readings appear in background service worker console at ~1-second intervals when webcam is active"
    - "First successful reading auto-captures a baseline spread value"
    - "Each reading includes spread, baseline, and ratio (current/baseline)"
    - "Detection stops cleanly when camera is disabled"
  artifacts:
    - path: "offscreen/offscreen.html"
      provides: "ES module script tag for offscreen.js"
      contains: "type=\"module\""
    - path: "offscreen/offscreen.js"
      provides: "FaceLandmarker init, detection loop, distance extraction, baseline capture"
      contains: "DISTANCE_READING"
    - path: "background.js"
      provides: "DISTANCE_READING message handler with console logging"
      contains: "DISTANCE_READING"
  key_links:
    - from: "offscreen/offscreen.js"
      to: "vendor/vision_bundle.mjs"
      via: "ES module import"
      pattern: "import.*vision_bundle"
    - from: "offscreen/offscreen.js"
      to: "background.js"
      via: "port.postMessage DISTANCE_READING"
      pattern: "DISTANCE_READING"
    - from: "offscreen/offscreen.js"
      to: "wasm/"
      via: "FilesetResolver.forVisionTasks(chrome.runtime.getURL('wasm'))"
      pattern: "forVisionTasks"
---

<objective>
Wire FaceLandmarker detection into the offscreen document and report distance readings to the background service worker.

Purpose: This is the core detection pipeline — the offscreen document runs MediaPipe inference at ~1s intervals, extracts eye landmark spread as a distance proxy, auto-captures baseline on first reading, and sends ratio data to background.js for future zoom control.
Output: Working detection loop producing DISTANCE_READING messages visible in the service worker console.
</objective>

<execution_context>
@/Users/dusan/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dusan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-detection-pipeline/02-RESEARCH.md
@.planning/phases/01-foundation/01-01-SUMMARY.md
@.planning/phases/02-detection-pipeline/02-01-SUMMARY.md
@offscreen/offscreen.html
@offscreen/offscreen.js
@background.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add FaceLandmarker detection loop to offscreen document</name>
  <files>
    offscreen/offscreen.html
    offscreen/offscreen.js
  </files>
  <action>
    **offscreen.html:**
    Change `<script src="offscreen.js"></script>` to `<script type="module" src="offscreen.js"></script>`.

    **offscreen.js:**
    Rewrite as an ES module. Keep ALL existing functionality (port connection, heartbeat, start/stop camera, reconnect logic) and ADD MediaPipe detection:

    1. At top of file, add static import:
       `import { FilesetResolver, FaceLandmarker } from '../vendor/vision_bundle.mjs';`

    2. Add module-level state variables:
       - `let faceLandmarker = null;`
       - `let baseline = null;`
       - `let detectionInterval = null;`

    3. Add `async function initFaceLandmarker()`:
       - `const vision = await FilesetResolver.forVisionTasks(chrome.runtime.getURL('wasm'));`
       - `faceLandmarker = await FaceLandmarker.createFromOptions(vision, { baseOptions: { modelAssetPath: chrome.runtime.getURL('model/face_landmarker.task') }, runningMode: 'VIDEO', numFaces: 1 });`
       - Log: `console.log('ZoomMe: FaceLandmarker initialized');`

    4. Add `function extractSpread(result)`:
       - Return null if `!result.faceLandmarks?.length`
       - Get landmarks[0], compute `Math.abs(lm[263].x - lm[33].x)`
       - Return the spread value

    5. Add `function startDetectionLoop()`:
       - `detectionInterval = setInterval(() => { ... }, 1000);`
       - Inside: check `if (!faceLandmarker || video.readyState < 2) return;`
       - Call `faceLandmarker.detectForVideo(video, performance.now())`
       - Extract spread via `extractSpread(result)`
       - If spread is null, return (no face detected)
       - If `baseline === null`, set `baseline = spread` and log it
       - Compute `ratio = spread / baseline`
       - Send via port: `backgroundPort.postMessage({ type: 'DISTANCE_READING', spread, baseline, ratio });`

    6. Add `function stopDetectionLoop()`:
       - `clearInterval(detectionInterval); detectionInterval = null;`
       - `baseline = null;` (reset baseline on stop so next enable recalibrates)

    7. Modify `startCamera()`:
       - AFTER `backgroundPort.postMessage({ type: 'CAMERA_READY' })`, call `initFaceLandmarker().then(() => startDetectionLoop());`
       - This keeps CAMERA_READY instant (camera is ready) and loads MediaPipe in parallel.

    8. Modify `stopCamera()`:
       - Call `stopDetectionLoop()` BEFORE stopping the stream tracks.

    IMPORTANT: The `video` element reference (`document.getElementById('video')`) must be inside the module scope. Since ES modules are deferred by default, `document.getElementById('video')` will work without DOMContentLoaded. Keep the existing `connectToBackground()` call at the bottom of the module.

    Do NOT use `import()` dynamic import — use static import with relative path `'../vendor/vision_bundle.mjs'`.
  </action>
  <verify>
    1. Read offscreen.html — confirm `type="module"` on script tag
    2. Read offscreen.js — confirm:
       - Static import from '../vendor/vision_bundle.mjs'
       - initFaceLandmarker function with FilesetResolver + FaceLandmarker
       - startDetectionLoop with setInterval at 1000ms
       - stopDetectionLoop clears interval and resets baseline
       - extractSpread uses landmarks 33 and 263
       - DISTANCE_READING sent via backgroundPort
       - startCamera calls initFaceLandmarker then startDetectionLoop after CAMERA_READY
       - stopCamera calls stopDetectionLoop before stopping tracks
       - All original functionality preserved (port, heartbeat, reconnect, start/stop)
  </verify>
  <done>offscreen.js imports MediaPipe, initializes FaceLandmarker after camera starts, runs detection at 1s intervals, auto-captures baseline, and sends DISTANCE_READING with spread/baseline/ratio to background.</done>
</task>

<task type="auto">
  <name>Task 2: Handle DISTANCE_READING in background service worker</name>
  <files>background.js</files>
  <action>
    In background.js, inside the `port.onMessage.addListener` callback (where CAMERA_READY, CAMERA_ERROR, and HEARTBEAT are handled), add a handler for DISTANCE_READING:

    ```javascript
    } else if (msg.type === 'DISTANCE_READING') {
      console.log('ZoomMe: distance reading', {
        spread: msg.spread.toFixed(4),
        baseline: msg.baseline.toFixed(4),
        ratio: msg.ratio.toFixed(3)
      });
    }
    ```

    This is the minimum handler for Phase 2 — it logs readings to the service worker console. Phase 3 will replace this with actual zoom logic.

    Do NOT modify any other part of background.js. The handler goes in the existing onMessage listener chain.
  </action>
  <verify>
    Read background.js — confirm DISTANCE_READING handler exists in port.onMessage listener, logs spread/baseline/ratio to console.
  </verify>
  <done>background.js receives and logs DISTANCE_READING messages from the offscreen document.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 3: Verify detection pipeline end-to-end</name>
  <files>none</files>
  <action>
    Human verification checkpoint. What was built:
    MediaPipe FaceLandmarker detection running in offscreen document, reporting distance readings to background service worker at ~1-second intervals with auto-baseline capture.

    Steps to verify:
    1. Open chrome://extensions, reload the ZoomMe extension (or load unpacked if not already loaded)
    2. Open the service worker console (click "Inspect views: service worker" on the extension card)
    3. Click the ZoomMe popup icon and check "Use ZoomMe" — webcam LED should turn on
    4. In the service worker console, watch for:
       - "ZoomMe: FaceLandmarker initialized" (appears once after ~1-3s)
       - "ZoomMe: distance reading { spread: 0.XXXX, baseline: 0.XXXX, ratio: 1.XXX }" (appears every ~1 second)
    5. The first reading should show ratio close to 1.000 (baseline captured)
    6. Move your head closer to the screen — ratio should increase (>1.0)
    7. Move your head farther — ratio should decrease (<1.0)
    8. Uncheck "Use ZoomMe" — readings should stop, webcam LED turns off
    9. Re-enable — new baseline should be captured (fresh calibration)
    10. CPU usage should remain reasonable (check Activity Monitor / Task Manager — not pegging a core)
  </action>
  <verify>Human confirms all 10 verification steps pass.</verify>
  <done>Detection pipeline verified end-to-end: readings appear at ~1s intervals, baseline auto-captured, ratio changes with distance, clean stop/restart.</done>
</task>

</tasks>

<verification>
Phase 2 success criteria check:
1. Distance readings appear in the service worker console at ~1-second intervals when webcam is active
2. First reading auto-captures baseline (ratio starts at ~1.0)
3. CPU usage stays reasonable (inference throttled to 1/second, not every frame)
</verification>

<success_criteria>
The offscreen document runs MediaPipe FaceLandmarker inference at ~1s intervals, extracts eye landmark spread as distance proxy, auto-captures baseline on first reading, and reports spread/baseline/ratio to the background service worker via DISTANCE_READING messages. All visible in service worker console. Detection stops cleanly on disable.
</success_criteria>

<output>
After completion, create `.planning/phases/02-detection-pipeline/02-02-SUMMARY.md`
</output>
